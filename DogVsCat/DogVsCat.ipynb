{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pinka\\AppData\\Roaming\\Python\\Python312\\site-packages\\PIL\\TiffImagePlugin.py:890: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Removed 0 corrupted images.\n",
      "Found 20000 images belonging to 2 classes.\n",
      "Found 4998 images belonging to 2 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pinka\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n",
      "C:\\Users\\pinka\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 218ms/step - accuracy: 0.6393 - loss: 0.6249 - val_accuracy: 0.7500 - val_loss: 0.5090\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 220ms/step - accuracy: 0.7603 - loss: 0.4992 - val_accuracy: 0.7783 - val_loss: 0.4707\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m758s\u001b[0m 1s/step - accuracy: 0.8077 - loss: 0.4285 - val_accuracy: 0.7805 - val_loss: 0.4649\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 205ms/step - accuracy: 0.8255 - loss: 0.3873 - val_accuracy: 0.7970 - val_loss: 0.4468\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 207ms/step - accuracy: 0.8515 - loss: 0.3354 - val_accuracy: 0.7945 - val_loss: 0.4573\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 209ms/step - accuracy: 0.8718 - loss: 0.2955 - val_accuracy: 0.7840 - val_loss: 0.4791\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 212ms/step - accuracy: 0.8887 - loss: 0.2610 - val_accuracy: 0.7791 - val_loss: 0.5074\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 212ms/step - accuracy: 0.9033 - loss: 0.2346 - val_accuracy: 0.8067 - val_loss: 0.4669\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 212ms/step - accuracy: 0.9152 - loss: 0.2080 - val_accuracy: 0.7996 - val_loss: 0.4977\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 216ms/step - accuracy: 0.9164 - loss: 0.1978 - val_accuracy: 0.7940 - val_loss: 0.5533\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.43      0.46      2499\n",
      "           1       0.50      0.57      0.53      2499\n",
      "\n",
      "    accuracy                           0.50      4998\n",
      "   macro avg       0.50      0.50      0.50      4998\n",
      "weighted avg       0.50      0.50      0.50      4998\n",
      "\n",
      "[[1078 1421]\n",
      " [1080 1419]]\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 95ms/step - accuracy: 0.7838 - loss: 0.5631\n",
      "Model Accuracy:  0.7931263446807861\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpinka\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDigicrome\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDeepLearning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDogVsCat\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPetImages\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCat\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m19.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m150\u001b[39m))\n\u001b[1;32m---> 52\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(img) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     53\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m     54\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "cnt=0\n",
    "test_dir=r\"C:\\\\Users\\\\pinka\\\\Digicrome\\\\DeepLearning\\\\DogVsCat\\\\PetImages\"\n",
    "for root, dirs, files in os.walk(test_dir):\n",
    "    for file in files:\n",
    "        img_path = os.path.join(root, file)\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            img.verify()\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print('Bad file:', img_path)\n",
    "            os.remove(img_path)\n",
    "            cnt += 1\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Removed {cnt} corrupted images.\")\n",
    "dat_generator=ImageDataGenerator(rescale=1./255,validation_split=0.2)\n",
    "train_data=dat_generator.flow_from_directory(test_dir,target_size=(150,150),batch_size=32,class_mode='binary',subset='training')\n",
    "val_data=dat_generator.flow_from_directory(test_dir,target_size=(150,150),batch_size=32,class_mode='binary',subset='validation')\n",
    "model=Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    #Conv2D(128, (3,3), activation='relu'),\n",
    "    #MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    #Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = (model.predict(val_data) > 0.5).astype(\"int32\")\n",
    "y_true = val_data.classes\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\"Model Accuracy: \", model.evaluate(val_data)[1])\n",
    "\n",
    "#test a model with a custom image\n",
    "img_path = r\"C:\\\\Users\\\\pinka\\\\Digicrome\\\\DeepLearning\\DogVsCat\\\\PetImages\\\\Cat\\\\19.jpg\"\n",
    "img = Image.open(img_path).resize((150, 150))\n",
    "img = np.array(img) / 255.0\n",
    "img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "prediction = model.predict(img)\n",
    "print(\"Predicted class:\", \"Dog\" if prediction[0][0] > 0.5 else \"Cat\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
